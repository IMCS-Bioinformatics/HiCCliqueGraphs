{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demonstration notebook to run different randomization methods on 1 dataset, and plot results.\n",
    "\n",
    "Tissue pcHi-C dataset is used as an example.\n",
    "\n",
    "The dataset is stored in our own format that is used in our other studies. In this notebook a large part of the code is dedicated to extract the link lists of one tissue type, and later to store the randomized links in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from randomizerMain import RandLauncher\n",
    "from addingNoiseOurVersion import createNoise\n",
    "\n",
    "from copy import deepcopy\n",
    "import json, csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code allows to perform all 3 types of randomization - the version that preserves both node degrees and link lengths, then the naive version of it (only preserves node degrees), and noise addition.\n",
    "\n",
    "Finally, the randomized datasets are analyzed and plots are plotted for them.\n",
    "\n",
    "As an example, data for chr4,8,12,16,20 is included in this repository. All chromosome data is available under \"Hi-C datasets seperated by chromosome\" in http://susurs.mii.lu.lv/HiCData/\n",
    "\n",
    "In the cell below, it is possible to modify the dataset that is randomized.\n",
    "To use it on your own data, you need to prepare a simple interaction list. See example in main.py\n",
    "\n",
    "The template right now will need ~3h to run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose dataset to randomize\n",
    "available_datasets = [\"Tissue pcHi-C\", \"Tissue Hi-C\", \"Blood pcHI-C\", \"Schwarzer Hi-C\"]\n",
    "chosenDataset = \"Tissue pcHi-C\" #Choose one of the above\n",
    "NUMBER_OF_REPEATS = 2 #if 2 is selected, a randomization with each set of parameters will be performed twice. \n",
    "#Expected running time: 3 hours; set NUMBER_OF_REPEATS to 1 to cut the time 2 times\n",
    "\n",
    "dirs_with_data_files = {\n",
    "    \"Tissue pcHi-C\": \"../../sampleData/Tissue pcHiC\",\n",
    "    \"Tissue Hi-C\": \"../../sampleData/Tissue HiC\",\n",
    "    \"Blood pcHi-C\": \"../../sampleData/Blood pcHiC\",\n",
    "    \"Schwarzer Hi-C\": \"../../sampleData/Schwarzer HiC\",\n",
    "}\n",
    "\n",
    "\n",
    "templ = {\n",
    "    \"DS\": chosenDataset, #short name for the dataset\n",
    "    \"dir\": dirs_with_data_files[chosenDataset], # dir is a path to a directory that contains files with data files for chromosomes\n",
    "    \"number of iters\": NUMBER_OF_REPEATS, #how many times to independently repeat the randomization for each graph\n",
    "    \"chrs\" : [\"chr4\", \"chr8\", \"chr12\", \"chr16\", \"chr20\"], #List of chromosomes to process. These chromosomes are included in the repository for demo purposes. Data files with all chromosomes are available on http://susurs.mii.lu.lv/HiCData/\n",
    "    \"randomization degree targets\": [0.25, 0.5, 0.75], #the parameter q for the proportion of links to attempt to randomize\n",
    "    \"result directory\": \"./demo results\", #all randomized files will be stored there\n",
    "\n",
    "    \"number of iters naive\": NUMBER_OF_REPEATS, #how many times to independently repeat the naive randomization for each graph\n",
    "    \"randomization degree targets naive\": [0.2, 0.3, 0.4, 0.5, 0.6], #the parameter q for the proportion of links to attempt to randomize\n",
    "    \"result directory naive\": \"./demo results\", #all randomized files will be stored there\n",
    "\n",
    "    \"number of iters noise\": NUMBER_OF_REPEATS,\n",
    "    \"noise levels\": [0.3, 0.6, 0.9, 1.2],\n",
    "    \"random ligation noise level\": [0.0, 0.5, 1.0], #predefined, do not modify [0, 0.5, 1.0] because plotting functions expect these proportions of noise\n",
    "    \"result directory noise\": \"./demo results\", \n",
    "}\n",
    "\n",
    "if chosenDataset==\"Schwarzer Hi-C\":\n",
    "    templ[\"chrs\"]=[\"chr4\", \"chr8\", \"chr12\", \"chr16\"] #because chr20 is unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions\n",
    "lastTime = time.time()\n",
    "def measureTime():\n",
    "    global lastTime\n",
    "    delta = time.time()-lastTime\n",
    "    lastTime = time.time()\n",
    "    return delta\n",
    "#make a dir for a file that is about to be saved\n",
    "def mkDir(fullPath):\n",
    "    # Extract the directory path\n",
    "    dirPath = os.path.dirname(fullPath)\n",
    "    if not os.path.exists(dirPath):\n",
    "        os.makedirs(dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for jjj in range(templ[\"number of iters\"]):\n",
    "    #for iCh in list(range(11,23))+[\"X\"]:\n",
    "    for Q in templ[\"randomization degree targets\"]:\n",
    "        for ch in templ[\"chrs\"]:\n",
    "            print(ch)\n",
    "            fn = f'{templ[\"dir\"]}/{ch}.json'\n",
    "            with open(fn, 'r') as f:\n",
    "                U = json.load(f)\n",
    "            newU = deepcopy(U) #links will change in this newU\n",
    "            ####### This portion is to extract the interaction list for each tissue type #######\n",
    "            newLinkBits = {} #keys - tuples of links iin segment indeces, values - bitmaps\n",
    "\n",
    "            tissueBits = U[\"tissueData\"][\"tissueBits\"]\n",
    "            allLinks = U[\"chrValues\"][ch][\"links\"]\n",
    "            allSegments = U[\"chrValues\"][ch][\"segments\"]\n",
    "            measureTime()\n",
    "            for tis,tisBit in tissueBits.items(): #iterate over available tissue types in the dataset\n",
    "                tissueLinks = [link for i,link in enumerate(allLinks) if ((link[2]&tisBit)==tisBit)]\n",
    "                links = [[allSegments[A][0], allSegments[B][0]] for [A,B,bit] in tissueLinks]\n",
    "                links = sorted(links, key=lambda x: (x[0],x[1]))\n",
    "                #links are the extracted interactions for the given tissue\n",
    "\n",
    "                obj = {\n",
    "                    \"useLinkList\": True,\n",
    "                    \"inputFn\": links,\n",
    "                    \"outputFn\": f'{templ[\"result directory\"]}/{templ[\"DS\"]}-demoRand-{ch}-{tis}-{str(Q)}-{jjj}.csv',\n",
    "                    \"q\": Q,\n",
    "                    \"runNaiveRandomization\": False,\n",
    "                    \"coolRelated\":{},\n",
    "                }\n",
    "                mkDir(obj[\"outputFn\"])\n",
    "\n",
    "\n",
    "                R = RandLauncher(obj) #!!! Randomization is performed here !!!\n",
    "                resFn = obj[\"outputFn\"]\n",
    "                #Randomized links are stored in the csv file resFn\n",
    "\n",
    "                ##### Now, all code below is to save the randomized links to the same format that is used in our studies and that can be processed and analyzed further\n",
    "                with open(resFn, newline='') as csvfile:\n",
    "                    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                    L = list(spamreader)\n",
    "                L = [[int(A),int(B)] for [A,B] in L]\n",
    "\n",
    "                startLocusToSegmentIndex = {allSegments[i][0]:i for i,_ in enumerate(allSegments)}\n",
    "                def getSegmentIndex(startLocus):\n",
    "                    return startLocusToSegmentIndex[startLocus]\n",
    "                \n",
    "                for link in L:\n",
    "                    (A,B) = sorted((getSegmentIndex(link[0]),getSegmentIndex(link[1])))\n",
    "                    if (A,B) not in newLinkBits:\n",
    "                        newLinkBits[(A,B)]=0\n",
    "                    newLinkBits[(A,B)]=(newLinkBits[(A,B)]|tisBit)\n",
    "            newLinkKeys = sorted(list(newLinkBits.keys()), key=lambda x: (x[0],x[1]))\n",
    "            newLinks = []\n",
    "            for key in newLinkKeys:\n",
    "                newLinks.append([key[0], key[1], newLinkBits[key]])\n",
    "            newU[\"chrValues\"][ch][\"links\"] = newLinks\n",
    "\n",
    "            dumpedFn = f\"{templ['result directory']}/randomized-{templ['DS']}-demo-{ch}-{obj['q']}-{jjj}.json\"\n",
    "            with open(dumpedFn, 'w') as file:\n",
    "                json.dump(newU, file)\n",
    "            print(dumpedFn)\n",
    "            times.append([dumpedFn, measureTime()])\n",
    "\n",
    "            with open(f\"{templ['result directory']}/timesRand-{templ['DS']}.csv\", 'w', newline='') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                        quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                spamwriter.writerows(times)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, naive randomization is to be run\n",
    "times = []\n",
    "for jjj in range(templ[\"number of iters naive\"]):\n",
    "    #for iCh in list(range(11,23))+[\"X\"]:\n",
    "    for Q in templ[\"randomization degree targets naive\"]:\n",
    "        for ch in templ[\"chrs\"]:\n",
    "            print(ch)\n",
    "            fn = f'{templ[\"dir\"]}/{ch}.json'\n",
    "            with open(fn, 'r') as f:\n",
    "                U = json.load(f)\n",
    "            newU = deepcopy(U) #links will change in this newU\n",
    "            ####### This portion is to extract the interaction list for each tissue type #######\n",
    "            newLinkBits = {} #keys - tuples of links iin segment indeces, values - bitmaps\n",
    "\n",
    "            tissueBits = U[\"tissueData\"][\"tissueBits\"]\n",
    "            allLinks = U[\"chrValues\"][ch][\"links\"]\n",
    "            allSegments = U[\"chrValues\"][ch][\"segments\"]\n",
    "            measureTime()\n",
    "            for tis,tisBit in tissueBits.items(): #iterate over available tissue types in the dataset\n",
    "                tissueLinks = [link for i,link in enumerate(allLinks) if ((link[2]&tisBit)==tisBit)]\n",
    "                links = [[allSegments[A][0], allSegments[B][0]] for [A,B,bit] in tissueLinks]\n",
    "                links = sorted(links, key=lambda x: (x[0],x[1]))\n",
    "                #links are the extracted interactions for the given tissue\n",
    "\n",
    "                obj = {\n",
    "                    \"useLinkList\": True,\n",
    "                    \"inputFn\": links,\n",
    "                    \"outputFn\": f'{templ[\"result directory\"]}/{templ[\"DS\"]}-demoNaive-{ch}-{tis}-{str(Q)}-{jjj}.csv',\n",
    "                    \"q\": Q,\n",
    "                    \"runNaiveRandomization\": True, #true to run naive version\n",
    "                    \"coolRelated\":{},\n",
    "                }\n",
    "                mkDir(obj[\"outputFn\"])\n",
    "\n",
    "\n",
    "                R = RandLauncher(obj) #!!! Randomization is performed here !!!\n",
    "                resFn = obj[\"outputFn\"]\n",
    "                #Randomized links are stored in the csv file resFn\n",
    "\n",
    "                ##### Now, all code below is to save the randomized links to the same format that is used in our studies and that can be processed and analyzed further\n",
    "                with open(resFn, newline='') as csvfile:\n",
    "                    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                    L = list(spamreader)\n",
    "                L = [[int(A),int(B)] for [A,B] in L]\n",
    "\n",
    "                startLocusToSegmentIndex = {allSegments[i][0]:i for i,_ in enumerate(allSegments)}\n",
    "                def getSegmentIndex(startLocus):\n",
    "                    return startLocusToSegmentIndex[startLocus]\n",
    "                \n",
    "                for link in L:\n",
    "                    (A,B) = sorted((getSegmentIndex(link[0]),getSegmentIndex(link[1])))\n",
    "                    if (A,B) not in newLinkBits:\n",
    "                        newLinkBits[(A,B)]=0\n",
    "                    newLinkBits[(A,B)]=(newLinkBits[(A,B)]|tisBit)\n",
    "            newLinkKeys = sorted(list(newLinkBits.keys()), key=lambda x: (x[0],x[1]))\n",
    "            newLinks = []\n",
    "            for key in newLinkKeys:\n",
    "                newLinks.append([key[0], key[1], newLinkBits[key]])\n",
    "            newU[\"chrValues\"][ch][\"links\"] = newLinks\n",
    "\n",
    "            dumpedFn = f\"{templ['result directory']}/naive-{templ['DS']}-demo-{ch}-{obj['q']}-{jjj}.json\"\n",
    "            with open(dumpedFn, 'w') as file:\n",
    "                json.dump(newU, file)\n",
    "            print(dumpedFn)\n",
    "            times.append([dumpedFn, measureTime()])\n",
    "\n",
    "            with open(f\"{templ['result directory']}/timesNaive-{templ['DS']}.csv\", 'w', newline='') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                        quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                spamwriter.writerows(times)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, noise will be added to the dataset\n",
    "for ch in templ[\"chrs\"]:\n",
    "    sourceFn = f'{templ[\"dir\"]}/{ch}.json'\n",
    "    for noiseToAdd in templ[\"noise levels\"]:\n",
    "        for rln in templ[\"random ligation noise level\"]:\n",
    "            for jjj in range(templ[\"number of iters noise\"]):\n",
    "                createNoise(sourceFn, ch, noiseToAdd, rln, jjj, templ[\"result directory noise\"], templ[\"DS\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the created files are analyzed in order to produce box plots to summarize results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions\n",
    "\n",
    "def makeAdjAndBitsOfLink(links):\n",
    "    # creates an adj structure for all links and creates a dictionary bitsOfLink\n",
    "    # bitsOfLink[(A,B)] == the bitmap of the link\n",
    "    adj = dict()\n",
    "    bitsOfLink = dict()\n",
    "    for link in links:\n",
    "        [A, B, bitmap, *pv] = link\n",
    "        if (B<=A): \n",
    "            #print (\"Warning: got an unsorted Link in makeAdj method!\")\n",
    "            #print(link)\n",
    "            tmm=A\n",
    "            A=B\n",
    "            B=tmm\n",
    "        if A not in adj: adj[A] = set()\n",
    "        adj[A].add(B)\n",
    "        if B not in adj: adj[B] = set()\n",
    "        adj[B].add(A)\n",
    "        bitsOfLink[(A,B)] = bitmap\n",
    "    return [adj, bitsOfLink]\n",
    "def getC3(links):\n",
    "    # finds all triangles in current chr. At least one tissue must be shared among each link of the triangle\n",
    "    # result is a list of tuples (A, B, C, bit), where A, B, C are sorted asc. and where bit is the max bitmap shared among all links (AB)&(AC)&(BC)\n",
    "    # only those triangles are kept that have at least one common tissue in all 3 links\n",
    "    # uses links == self.links\n",
    "    # links argumentthat is used here has list of links in form [A, B, bitmap] for one chromosome\n",
    "    \n",
    "    [adj, bitsOfLinks] = makeAdjAndBitsOfLink(links) \n",
    "    #adj[A] == set of all other vertices that are adjacent to A\n",
    "    #bitsOfLink[(A,B)] == the bitmap of the link (A,B)\n",
    "    cliques = set() #{(A,B,C), (), ()}\n",
    "    \n",
    "    # Algorithm to find C3: take vertice A. setOfBs has all its direct neighbors (B>A to get A<B<C in result and get rid of duplicates)\n",
    "    # Each node in setOfBs can be on a C3. If A-B is in some C3, then there exists a C that is both adjacent to B and to A.\n",
    "    # Nodes that are adjacent to A are stored in setOfBs. Nodes that are adjacent to current B are stored in setOfCs.\n",
    "    # Taking intersection - if it is non-empty, for each C, A-B-C is a triangle (clique)\n",
    "    for A in adj.keys():\n",
    "        setOfBs = adj[A]\n",
    "        setOfBs = set(el for el in setOfBs if el>A)\n",
    "        for B in setOfBs:\n",
    "            setOfCs = adj[B]\n",
    "            setOfCs = set(el for el in setOfCs if el>B)\n",
    "            \n",
    "            goodCs = setOfCs.intersection(setOfBs)\n",
    "            for C in goodCs:\n",
    "                [A,B,C] = sorted([A,B,C])\n",
    "                bits = ( bitsOfLinks[(A,B)] & bitsOfLinks[(B,C)] & bitsOfLinks[(A,C)])\n",
    "                if bits>0: #why 0? if want all CL3 not looking at tissue shared, can ignore this if\n",
    "                    cliques.add((A,B,C,bits))\n",
    "    del adj\n",
    "    del bitsOfLinks\n",
    "    return cliques \n",
    "\n",
    "import cooler\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def getCool(links, ch, resolution, resultCoolFn):\n",
    "    rows=[[ch,link[0],link[0]+resolution, link[1], link[1]+resolution, 10] for link in links]\n",
    "    chr_rows = {}\n",
    "    for row in rows:\n",
    "        if row[0] not in chr_rows:\n",
    "            chr_rows[row[0]] = []\n",
    "        chr_rows[row[0]].append(row)\n",
    "    vertices = set()\n",
    "    for chr in chr_rows:\n",
    "        curr_rows = chr_rows[chr]\n",
    "        for row in curr_rows:\n",
    "            vertices.add((row[0], int(row[1]), int(row[2])))\n",
    "            vertices.add((row[0], int(row[3]), int(row[4])))\n",
    "    vertices = sorted(list(vertices))\n",
    "    vertice_to_id = {x: i for i, x in enumerate(vertices)}\n",
    "    edges = {}\n",
    "    for chr in chr_rows:\n",
    "        curr_rows = chr_rows[chr]\n",
    "        for row in curr_rows:\n",
    "            bin1 = (row[0], int(row[1]), int(row[2]))\n",
    "            bin2 = (row[0], int(row[3]), int(row[4]))\n",
    "            if bin1 > bin2:\n",
    "                bin1, bin2 = bin2, bin1\n",
    "            if (bin1, bin2) not in edges:\n",
    "                edges[(bin1, bin2)] = int(float(row[5])*10)\n",
    "            else:\n",
    "                edges[(bin1, bin2)] = max(\n",
    "                    edges[(bin1, bin2)], int(float(row[5])*10))\n",
    "    edge_list = []\n",
    "    for x in edges:\n",
    "        bin1 = vertice_to_id[x[0]]\n",
    "        bin2 = vertice_to_id[x[1]]\n",
    "        edge_list.append((bin1, bin2, edges[x]))\n",
    "        # edges.add(\n",
    "        #     (vertice_to_id[bin1], vertice_to_id[bin2], row[5]))\n",
    "    edges = sorted(edge_list)\n",
    "    bins = {\n",
    "        \"chrom\": [],\n",
    "        \"start\": [],\n",
    "        \"end\": []\n",
    "    }\n",
    "    for v in vertices:\n",
    "        bins['chrom'].append(v[0])\n",
    "        bins['start'].append(v[1])\n",
    "        bins['end'].append(v[2])\n",
    "    bins_df = pd.DataFrame(bins)\n",
    "\n",
    "    pixels = {\n",
    "        \"bin1_id\": [],\n",
    "        \"bin2_id\": [],\n",
    "        \"count\": []\n",
    "    }\n",
    "    for e in edges:\n",
    "        pixels['bin1_id'].append(e[0])\n",
    "        pixels['bin2_id'].append(e[1])\n",
    "        pixels['count'].append(e[2])\n",
    "    pixels_df = pd.DataFrame(pixels)\n",
    "    cooler.create_cooler(\n",
    "        resultCoolFn, bins_df, pixels_df)\n",
    "    c = cooler.Cooler(resultCoolFn)\n",
    "    px = c.pixels()[:10]\n",
    "    #print(px)\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import hicrep\n",
    "from hicrep.utils import readMcool\n",
    "from hicrep import hicrepSCC\n",
    "\n",
    "def doHiCRep(fcool1, fcool2, chrs):\n",
    "    \n",
    "    cool1, binSize1 = readMcool(fcool1, -1)\n",
    "    cool2, binSize2 = readMcool(fcool2, -1)\n",
    "    # binSize1 and binSize2 will be set to the bin size built in the cool file\n",
    "    binSize = binSize1\n",
    "    # smoothing window half-size\n",
    "    h = 1\n",
    "\n",
    "    # maximal genomic distance to include in the calculation\n",
    "    dBPMax = 50000000\n",
    "\n",
    "    # whether to perform down-sampling or not \n",
    "    # if set True, it will bootstrap the data set # with larger contact counts to\n",
    "    # the same number of contacts as in the other data set; otherwise, the contact \n",
    "    # matrices will be normalized by the respective total number of contacts\n",
    "    bDownSample = False\n",
    "\n",
    "    # compute the SCC score\n",
    "    # this will result in a SCC score for each chromosome available in the data set\n",
    "    # listed in the same order as the chromosomes are listed in the input Cooler files\n",
    "    # scc = hicrepSCC(cool1, cool2, h, dBPMax, bDownSample)\n",
    "\n",
    "    # Optionally you can get SCC score from a subset of chromosomes\n",
    "    sccSub = hicrepSCC(cool1, cool2, h, dBPMax, bDownSample, np.array(chrs, dtype=str))\n",
    "    #print(sccSub)\n",
    "    return sccSub\n",
    "\n",
    "def getAvgNodeDegreeChange(linksO, linksR):\n",
    "    nodeDegrees = {} #original node degrees\n",
    "    for link in linksO:\n",
    "        if link[0] not in nodeDegrees:\n",
    "            nodeDegrees[link[0]]=0\n",
    "        nodeDegrees[link[0]]+=1\n",
    "        if link[1] not in nodeDegrees:\n",
    "            nodeDegrees[link[1]]=0\n",
    "        nodeDegrees[link[1]]+=1\n",
    "    nodeDegreesR = {} #original node degrees\n",
    "    for link in linksR:\n",
    "        if link[0] not in nodeDegreesR:\n",
    "            nodeDegreesR[link[0]]=0\n",
    "        nodeDegreesR[link[0]]+=1\n",
    "        if link[1] not in nodeDegreesR:\n",
    "            nodeDegreesR[link[1]]=0\n",
    "        nodeDegreesR[link[1]]+=1\n",
    "    diff = 0\n",
    "    allNodes = set(list(nodeDegrees.keys())+list(nodeDegreesR.keys()) )\n",
    "    for node in allNodes:\n",
    "        origDegree=0\n",
    "        if node in nodeDegrees:\n",
    "            origDegree = nodeDegrees[node]\n",
    "        randDegree=0\n",
    "        if node in nodeDegreesR:\n",
    "            randDegree = nodeDegreesR[node]\n",
    "        diff+=abs(randDegree-origDegree)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [] #results stored here\n",
    "header = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "def processFile(randFn, templ):\n",
    "    global header\n",
    "    filename = os.path.basename(randFn)\n",
    "    filename = filename.replace('.json', '')\n",
    "    filename = filename.replace('Hi-C', 'HiC')\n",
    "    parts = filename.split('-')\n",
    "    info = {\n",
    "        'type': parts[0],  # e.g., 'randomized'\n",
    "        'DS': parts[1],  # e.g., 'tispcHiC'\n",
    "        'chr': parts[3],  # e.g., 'chr2'\n",
    "        'rndDegree': float(parts[4]),  # e.g., 0.25\n",
    "        'iter': float(parts[5])  # e.g., 1\n",
    "    }\n",
    "    with open(randFn, 'r') as f:\n",
    "        R = json.load(f) #randomized file\n",
    "    with open(f'{templ[\"dir\"]}/{info[\"chr\"]}.json', 'r') as f:\n",
    "        U = json.load(f) #original file\n",
    "    iii=0\n",
    "    ch=info[\"chr\"]\n",
    "    oallLinks = U[\"chrValues\"][ch][\"links\"] # Original all links\n",
    "    oallSegments = U[\"chrValues\"][ch][\"segments\"] # Original all segments\n",
    "    rallLinks = R[\"chrValues\"][ch][\"links\"]\n",
    "    rallSegments = R[\"chrValues\"][ch][\"segments\"]\n",
    "    if \"tissueBits\" not in R: \n",
    "        tissueBits = R[\"tissueData\"][\"tissueBits\"]\n",
    "    else: \n",
    "        tissueBits = R[\"tissueBits\"]\n",
    "    for tis,tisBit in tissueBits.items():\n",
    "        rtissueLinks = [link for i,link in enumerate(rallLinks) if ((link[2]&tisBit)==tisBit)]\n",
    "        otissueLinks = [link for i,link in enumerate(oallLinks) if ((link[2]&tisBit)==tisBit)]\n",
    "\n",
    "        rSegments = set([A for [A,B,*pv] in rtissueLinks]+[B for [A,B,*pv] in rtissueLinks])\n",
    "        oSegments = set([A for [A,B,*pv] in otissueLinks]+[B for [A,B,*pv] in otissueLinks])\n",
    "\n",
    "        #commonLinks = set([sorted((A,B)) for [A,B,*pv] in rtissueLinks]).intersection(set([sorted((A,B)) for [A,B,*pv] in otissueLinks]))\n",
    "        a=set([(tuple(sorted((A,B)))) for [A,B,*pv] in otissueLinks])\n",
    "        b=set([(tuple(sorted((A,B)))) for [A,B,*pv] in rtissueLinks])\n",
    "        commonLinks =a.intersection(b)\n",
    "\n",
    "        rlLengths = [abs(rallSegments[B][1]-rallSegments[A][1]) for [A,B,*pvs] in rtissueLinks] \n",
    "        olLengths = [abs(oallSegments[B][1]-oallSegments[A][1]) for [A,B,*pvs] in otissueLinks] \n",
    "\n",
    "        actualOLinks = [[oallSegments[A][0],oallSegments[B][0]] for [A,B,*pvs] in otissueLinks]\n",
    "        actualRLinks = [[rallSegments[A][0],rallSegments[B][0]] for [A,B,*pvs] in rtissueLinks]\n",
    "        print(\"start mk cools\")\n",
    "        #make sure that all segmebnts in O are also in R.\n",
    "        #add some extra links\n",
    "        segsInR = [A for [A,B] in actualRLinks] + [B for [A,B] in actualRLinks]\n",
    "        segsInR = set(segsInR)\n",
    "        segsInO = [A for [A,B] in actualOLinks] + [B for [A,B] in actualOLinks]\n",
    "        segsInO = set(segsInO)\n",
    "        diff = segsInO.difference(segsInR)\n",
    "        for seg in diff:\n",
    "            actualRLinks.append([seg,seg])\n",
    "\n",
    "        getCool(actualOLinks, ch, 5000, \"tmpCoolO.cool\")\n",
    "        getCool(actualRLinks, ch, 5000, \"tmpCoolR.cool\")\n",
    "        print(\"made cools\")\n",
    "        hicreprez = doHiCRep(\"tmpCoolO.cool\", \"tmpCoolR.cool\", [ch])\n",
    "        print(f\"{hicreprez=}\")\n",
    "        hicrepScore = hicreprez[0]\n",
    "        \n",
    "\n",
    "        rC3 = getC3(rtissueLinks)\n",
    "        oC3 = getC3(otissueLinks)\n",
    "\n",
    "        rez = ks_2samp(olLengths, rlLengths)\n",
    "        stat = rez.statistic\n",
    "        pvalue=rez.pvalue\n",
    "\n",
    "        avgNodeDegreeChange = getAvgNodeDegreeChange(otissueLinks, rtissueLinks)\n",
    "\n",
    "        newRow = [\n",
    "            info[\"type\"],\n",
    "            info[\"DS\"],\n",
    "            info[\"chr\"],\n",
    "            info[\"rndDegree\"],\n",
    "            info[\"iter\"],\n",
    "            tis,\n",
    "            len(oSegments),\n",
    "            len(rSegments),\n",
    "            len(otissueLinks),\n",
    "            len(rtissueLinks),\n",
    "            len(commonLinks),\n",
    "            1-len(commonLinks)/len(rtissueLinks),\n",
    "            len(oC3),\n",
    "            len(rC3),\n",
    "            stat,\n",
    "            pvalue,\n",
    "            hicrepScore,\n",
    "            avgNodeDegreeChange,\n",
    "        ]\n",
    "        header = [\n",
    "            \"method\", \"DS\", \"chr\", \"rndDegree asked\", \"iter\", \"tis\", \n",
    "            \"original segments\", \"rnd segments\", \"original links\", \"rnd links\",\n",
    "            \"common links\", \"real rnd degree\",\n",
    "            \"original C3\", \"rnd C3\", \n",
    "            \"KS-stat\",\"KS-pvalue\",\n",
    "            \"HiCRep\",\n",
    "            \"total node degree change\",\n",
    "        ]\n",
    "        iii=0\n",
    "        L.append(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(L, header):\n",
    "    with open(f'combined-results-rnd-analysis-{templ[\"DS\"]}.csv', 'w', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow(header)\n",
    "        spamwriter.writerows(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "header = []\n",
    "counter = 0\n",
    "# Specify the directory path\n",
    "allPaths = set([templ[\"result directory\"], templ[\"result directory naive\"], templ[\"result directory noise\"]])\n",
    "for directory_path in allPaths:\n",
    "    # Get a list of files in the directory, sorted by file name\n",
    "    sorted_files = sorted(os.listdir(directory_path))\n",
    "    # Iterate over each file in the sorted list\n",
    "    for file_name in sorted_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        # Check if it's a file and not a directory (optional)\n",
    "        if os.path.isfile(file_path):\n",
    "            if \".csv\" in file_name or \".json\" not in file_name:\n",
    "                continue  #only process json files\n",
    "            print(file_path)\n",
    "            processFile(file_path, templ)\n",
    "            counter+=1\n",
    "            if L: print(L[-1])\n",
    "        if ((counter%10)==0): \n",
    "            store(L, header)\n",
    "    store(L,header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalues = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "delta = 0.05\n",
    "def find_closest_and_check(value, candidates=xvalues, delta=delta):\n",
    "    # Find the closest value in candidates to the given value\n",
    "    closest = min(candidates, key=lambda x: abs(x - value))\n",
    "    # Check if the closest value is within delta of the given value\n",
    "    if abs(closest - value) <= delta:\n",
    "        return closest\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "combined_df = pd.read_csv(f'combined-results-rnd-analysis-{templ[\"DS\"]}.csv')\n",
    "finalFn = f'combined-plottable-results-rnd-analysis-{templ[\"DS\"]}.csv'\n",
    "combined_df['boxSet'] = combined_df.apply(\n",
    "    lambda row: \n",
    "        \"Randomization preserving node degrees and link lengths\" if row['method']==\"randomized\" else(\n",
    "            \"Randomization preserving node degrees\" if row['method'] == \"naive\"\n",
    "                else (\n",
    "                    \"100% random ligation noise\" if row['iter'] == 1.0\n",
    "                    else (\n",
    "                        \"50/50 genomic distance effect noise and random ligation noise\" if row['iter'] == 0.5\n",
    "                        else (\n",
    "                            \"100% genomic distance effect noise\" if row['iter'] == 0.0\n",
    "                            else f\"noised {row['iter']} rln\"  # Fallback case if none of the above conditions are met\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), axis=1)\n",
    "\n",
    "combined_df['xvalue'] = combined_df['real rnd degree'].apply(find_closest_and_check)\n",
    "\n",
    "combined_df = combined_df[combined_df['xvalue'] != -1]\n",
    "\n",
    "combined_df['relative node degree change'] = combined_df['total node degree change']/combined_df['original segments']\n",
    "combined_df['relative C3 change'] = np.where(combined_df['original C3'] == 0, 1, combined_df['rnd C3']/combined_df['original C3'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "combined_df.to_csv(finalFn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FONT_SIZE=30\n",
    "\n",
    "def makePlotFromTemplate(templ, ax):\n",
    "    df1 = pd.read_csv(templ[\"fn\"])\n",
    "    # Define a grayscale color map for differentiation\n",
    "    color_map = {iter_val: plt.cm.Greys(np.linspace(0.3, 0.8, len(df1['boxSet'].unique()))[i]) for i, iter_val in enumerate(sorted(df1['boxSet'].unique()))}\n",
    "\n",
    "    # Initialize lists for plotting\n",
    "    data_to_plot = []\n",
    "    positions = []\n",
    "    colors = []\n",
    "    labels = []\n",
    "    line_data = {}\n",
    "    dash_styles = ['-', '--', '-.', ':', '-']  # Define dash styles for different iter_val\n",
    "    dash_styles = [\n",
    "        (5, 5),  # Dashed\n",
    "        (1, 3, 1, 3, 1, 3, 6, 3),  # Three dots and a dash\n",
    "        (2, 8),  # Dotted\n",
    "        (4, 2, 1, 2),  # Dash-dot\n",
    "        # (8, 4, 2, 4),  # Long dash, short dash\n",
    "        (),  # Solid\n",
    "        \n",
    "    ]\n",
    "\n",
    "    startYCoordinate=0\n",
    "    if \"startYCoordinate\" in templ:\n",
    "        startYCoordinate = templ[\"startYCoordinate\"]\n",
    "\n",
    "    # Adding an artificial box at the beginning for (0,0)\n",
    "    data_to_plot.append([startYCoordinate])  # Artificial data\n",
    "    positions.append(0)  # Position for the artificial box\n",
    "    colors.append('white')  # Color for the artificial box\n",
    "    labels.append('0')  # Label for the artificial box\n",
    "\n",
    "    for iter_val in sorted(df1['boxSet'].unique()):\n",
    "        df_iter = df1[df1['boxSet'] == iter_val]\n",
    "        rnd_degree_groups = df_iter.groupby('xvalue')['real rnd degree'].median().sort_values()\n",
    "        \n",
    "        # For connecting medians, start from (0,0)\n",
    "        x_positions = [0]\n",
    "        y_medians = [startYCoordinate]\n",
    "        \n",
    "        for rnd_degree, median_val in rnd_degree_groups.items():\n",
    "            group_data = df_iter[df_iter['xvalue'] == rnd_degree][templ[\"columnToPlot\"]].values\n",
    "            if iter_val in templ[\"removeXPoints\"]:\n",
    "                if rnd_degree in templ[\"removeXPoints\"][iter_val]:\n",
    "                    continue\n",
    "            if len(group_data) > 0:\n",
    "                data_to_plot.append(group_data)\n",
    "                median_position = median_val  # Adjusted to use median_val directly\n",
    "                positions.append(median_position)\n",
    "                colors.append(color_map[iter_val])\n",
    "                labels.append(f\"{median_val:.2f}\")\n",
    "                \n",
    "                # For lines\n",
    "                x_positions.append(median_position)\n",
    "                y_medians.append(np.median(group_data))\n",
    "        \n",
    "        # Store the line data\n",
    "        line_data[iter_val] = (x_positions, y_medians)\n",
    "\n",
    "    \n",
    "\n",
    "    # Customizing outlier properties with smaller dots and median line properties\n",
    "    flierprops = dict(marker='o', markerfacecolor='black', markersize=0.5, linestyle='none')\n",
    "    medianprops = dict(color='white', linewidth=1)  # Setting medians to white\n",
    "    showfliersOrNo = True\n",
    "    if \"showFliers\" in templ:\n",
    "        showfliersOrNo = templ[\"showFliers\"]\n",
    "    bp = ax.boxplot(data_to_plot, positions=positions, patch_artist=True, notch=True, widths=0.03, flierprops=flierprops, medianprops=medianprops, showfliers=showfliersOrNo)\n",
    "\n",
    "    # Apply colors to each box and adjust properties as necessary\n",
    "    for box, patch_color in zip(bp['boxes'], colors):\n",
    "        box.set_facecolor(patch_color)\n",
    "        box.set_edgecolor(patch_color)  # Set edge color to match the face color\n",
    "\n",
    "    # Apply colors to each box for both face and edge, and also match whiskers and caps\n",
    "    for i, box in enumerate(bp['boxes']):\n",
    "        patch_color = colors[i]\n",
    "        box.set_facecolor(patch_color)\n",
    "        box.set_edgecolor(patch_color)  # Set edge color to match the face color\n",
    "        # Set whiskers color\n",
    "        bp['whiskers'][i*2].set_color(patch_color)\n",
    "        bp['whiskers'][i*2 + 1].set_color(patch_color)\n",
    "        # Set caps color\n",
    "        bp['caps'][i*2].set_color(patch_color)\n",
    "        bp['caps'][i*2 + 1].set_color(patch_color)\n",
    "\n",
    "    for i, (iter_val, (x_pos, y_vals)) in enumerate(line_data.items()):\n",
    "        # Retrieve the dash style\n",
    "        dash_style = dash_styles[i % len(dash_styles)]\n",
    "        # Plot the line without specifying dash style\n",
    "        line, = ax.plot(x_pos, y_vals, color=color_map[iter_val], label=f'Iter {iter_val}')\n",
    "        # Apply dash style\n",
    "        if isinstance(dash_style, tuple):\n",
    "            line.set_dashes(dash_style)  # For custom dash patterns\n",
    "        elif isinstance(dash_style, str):\n",
    "            line.set_linestyle(dash_style)  # For predefined dash styles as strings\n",
    "\n",
    "    # Customizing the axes\n",
    "    ax.set_xticks(np.arange(0, max(positions) + 0.1, 0.1))  # Adjust based on your actual data range\n",
    "    ax.set_xticklabels([f\"{x:.1f}\" for x in np.arange(0, max(positions) + 0.1, 0.1)], rotation=45, ha='right')\n",
    "    ax.tick_params(axis='both', labelsize=18)  # Adjusts both x and y tick label sizes\n",
    "\n",
    "    if positions:\n",
    "        min_pos = min(positions)\n",
    "        max_pos = max(positions)\n",
    "        buffer = (max_pos - min_pos) * 0.05  # Add 5% buffer on each side\n",
    "        ax.set_xlim(min_pos - buffer, max_pos + buffer)\n",
    "\n",
    "    ax.set_ylabel(templ[\"ylabel\"], fontsize=18)\n",
    "    ax.set_xlabel(templ[\"xlabel\"], fontsize=18)\n",
    "    ax.set_title(templ[\"title\"], fontsize=20)\n",
    "\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "    legend_elements=[]\n",
    "    for i, iter_val in enumerate(sorted(df1['boxSet'].unique())):\n",
    "        dash_style = dash_styles[i % len(dash_styles)]\n",
    "        line = plt.Line2D([0], [0], color=color_map[iter_val], lw=2, label=df1[df1['boxSet'] == iter_val].iloc[0]['boxSet'])\n",
    "        if isinstance(dash_style, tuple):\n",
    "            line.set_dashes(dash_style)  # For custom dash patterns\n",
    "        else:\n",
    "            line.set_linestyle(dash_style)  # For predefined styles\n",
    "        legend_elements.append(line)\n",
    "    # # Update legend to include dash styles\n",
    "    # legend_elements = [\n",
    "    #     plt.Line2D([0], [0], color=color_map[iter_val], lw=2, linestyle=dash_styles[i % len(dash_styles)], label=df1[df1['boxSet'] == iter_val].iloc[0]['boxSet'])\n",
    "    #     for i, iter_val in enumerate(sorted(df1['boxSet'].unique()))\n",
    "    # ]\n",
    "    # showLegend=True\n",
    "    # if \"showLegend\" in templ :\n",
    "    #     showLegend = templ[\"showLegend\"]\n",
    "    # if showLegend:\n",
    "    #     ax.legend(handles=legend_elements, title=\"Randomization technique\")\n",
    "    ax.legend(handles=legend_elements, title=\"Randomization technique\", fontsize=12, handlelength=4)\n",
    "\n",
    "    # legend_elements = [plt.Line2D([0], [0], color=color_map[iter_val], lw=2, linestyle=dash_styles[i % len(dash_styles)], label=f'Iter {iter_val}') for i, iter_val in enumerate(sorted(df1['iter'].unique()))]\n",
    "    # ax.legend(handles=legend_elements, title=\"Iter Values and Styles\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sf=True\n",
    "templPlotTissuePCHiC_KS = {\n",
    "    \"columnToPlot\": 'KS-stat',\n",
    "    \"ylabel\": \"KS test statistic\",\n",
    "    \"xlabel\": \"Real randomization degree\",\n",
    "    \"title\": f\"2-sample Kolmogorov-Smirnov test statistic to compare\\n link length distribution of randomized graphs\",\n",
    "    \"removeXPoints\":    {\n",
    "                            \"Randomization preserving node degrees\": set([0.4, 0.5, 0.6, 0.7]),\n",
    "                            #\"100% random ligation noise\": set([0.4]), \n",
    "                        },\n",
    "    \"fn\": f'combined-plottable-results-rnd-analysis-{templ[\"DS\"]}.csv',\n",
    "    \"showFliers\": sf,\n",
    "}\n",
    "templPlotTissuePCHiC_ND = {\n",
    "    \"columnToPlot\": 'relative node degree change',\n",
    "    \"ylabel\": \"Relative node degree change\",\n",
    "    \"xlabel\": \"Real randomization degree\",\n",
    "    \"title\": f\"Average node degree change in randomized graphs\\n\",\n",
    "\n",
    "    \"removeXPoints\":    {},\n",
    "    \"fn\": f'combined-plottable-results-rnd-analysis-{templ[\"DS\"]}.csv',\n",
    "    \"showFliers\": sf,\n",
    "}\n",
    "\n",
    "templPlotTissuePCHiC_HR = {\n",
    "    \"columnToPlot\": 'HiCRep',\n",
    "    \"ylabel\": \"HiCRep reproducibility statistic\",\n",
    "    \"xlabel\": \"Real randomization degree\",\n",
    "    \"title\": f\"HiCRep reproducibility statistic change in randomized graphs\",\n",
    "    \"removeXPoints\":    {},\n",
    "    \"fn\": f'combined-plottable-results-rnd-analysis-{templ[\"DS\"]}.csv',\n",
    "    \"showFliers\": sf,\n",
    "    \"startYCoordinate\": 1.0,\n",
    "\n",
    "}\n",
    "sf=False\n",
    "templPlotTissuePCHiC_C3 = {\n",
    "    \"columnToPlot\": 'relative C3 change',\n",
    "    \"ylabel\": \"Relative C3 change\",\n",
    "    \"xlabel\": \"Real randomization degree\",\n",
    "    \"title\": \"Relative C3 change in randomized graphs\",\n",
    "    \"removeXPoints\":    {},\n",
    "    \"fn\": f'combined-plottable-results-rnd-analysis-{templ[\"DS\"]}.csv',\n",
    "    \"showFliers\": sf,\n",
    "    \"startYCoordinate\": 1.0,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18,14))\n",
    "\n",
    "makePlotFromTemplate(templPlotTissuePCHiC_KS, axs[0,0])\n",
    "makePlotFromTemplate(templPlotTissuePCHiC_ND, axs[0,1])\n",
    "makePlotFromTemplate(templPlotTissuePCHiC_HR, axs[1,0])\n",
    "makePlotFromTemplate(templPlotTissuePCHiC_C3, axs[1,1])\n",
    "\n",
    "#axs[0,0].legend().remove()\n",
    "# axs[0,1].legend().remove()\n",
    "# axs[1,0].legend().remove()\n",
    "# axs[1,1].legend().remove()\n",
    "\n",
    "# Add a supertitle to the figure\n",
    "fig.suptitle(f'{templ[\"DS\"]}', fontsize=FONT_SIZE)\n",
    "# Adjust layout to make room for the supertitle\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "\n",
    "# Now, save the figure to an EPS file with a specific DPI\n",
    "# fig.savefig(f'{templ[\"DS\"]}-4images.eps', format='eps', dpi=600)\n",
    "fig.savefig(f'{templ[\"DS\"]}-4images.png', dpi=600)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
